{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following http://modelai.gettysburg.edu/2013/cfr/cfr.pdf\n",
    "#### Exercise 2.5: RPS Equilibrium (regret minimization).  hand-coding the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r, p, s = 0, 1, 2\n",
    "\n",
    "training_iterations = 10000\n",
    "\n",
    "strategy_profile_sum = [0,0,0]\n",
    "\n",
    "player_one_regrets = [0,0,0]\n",
    "player_two_regrets = [0,0,0]\n",
    "\n",
    "def get_player_one_strategy():\n",
    "    return get_strategy_from_regrets(player_one_regrets)\n",
    "def get_player_two_strategy():\n",
    "    return get_strategy_from_regrets(player_two_regrets)\n",
    "def get_strategy_from_regrets(regrets):\n",
    "    nonzero_regrets = [max(0,x) for x in regrets]\n",
    "    if sum(nonzero_regrets) == 0:\n",
    "        return [1/len(nonzero_regrets) for x in nonzero_regrets]\n",
    "    return [x/sum(nonzero_regrets) for x in nonzero_regrets]\n",
    "def sample_action(distribution):\n",
    "    return random.choices(range(3), weights=distribution)[0]\n",
    "def get_player_one_value(a1, a2):\n",
    "    outcome = (a2-a1)%3\n",
    "    if outcome == 0:\n",
    "        return 0\n",
    "    elif outcome == 1:\n",
    "        return 1\n",
    "    elif outcome == 2:\n",
    "        return -1\n",
    "def get_utilities(opponent_action):\n",
    "    ans = []\n",
    "    for i in [0,1,2]:\n",
    "        ans.append(get_player_one_value(i,opponent_action))\n",
    "    return ans\n",
    "def accumulate(l1, l2):\n",
    "    assert(len(l1) == len(l2))\n",
    "    for i in range(len(l1)):\n",
    "        l1[i] += l2[i]\n",
    "    \n",
    "for i in range(training_iterations):\n",
    "    player_one_strat = get_player_one_strategy()\n",
    "    player_two_strat = get_player_two_strategy()\n",
    "    accumulate(strategy_profile_sum, player_one_strat)\n",
    "    player_one_action = sample_action(player_one_strat)\n",
    "    player_two_action = sample_action(player_two_strat)\n",
    "    player_one_value = get_player_one_value(player_one_action, player_two_action)\n",
    "    player_two_value = -player_one_value\n",
    "    \n",
    "    player_one_utilities = get_utilities(player_two_action)\n",
    "    player_one_regret = [x-player_one_value for x in player_one_utilities]\n",
    "    player_two_utilities = get_utilities(player_one_action)\n",
    "    player_two_regret = [x-player_two_value for x in player_two_utilities]\n",
    "    for j in range(len(player_one_regret)):\n",
    "        player_one_regrets[j] += player_one_regret[j]\n",
    "        player_two_regrets[j] += player_two_regret[j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3160.8931147910375, 3437.931922500671, 3401.1749627082904]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_profile_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.3161', '0.3438', '0.3401']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['{:.4}'.format(x/sum(strategy_profile_sum)) for x in strategy_profile_sum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it's hard to tell if that worked, since the equilbrium should be 1/3, 1/3, 1/3 and that could happen even if there was a bug.  Let's try this 538 riddler problem that I've always wanted to solve: https://fivethirtyeight.com/features/whats-the-optimal-way-to-play-horse/ (the riddler express problem) \n",
    "(by the way if I had done this while the contest was still running, this would be my submission: compute an equilibrium, then pick the highest-probability actions, then pick the best response to that would be.  submit that best response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again we'll just identify actions with their index integers: 0,1,2,3,4,5\n",
    "score_probability = [0.7,0.9,0.8,0.8,1,.9]\n",
    "num_of_actions = len(score_probability)\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, i_am_attacker):\n",
    "        self.cumulative_regrets = [0 for i in range(num_of_actions)]\n",
    "        self.strategy_profile_sum = [0 for i in range(num_of_actions)]\n",
    "        self.i_am_attacker = i_am_attacker\n",
    "    def sample_action(self):\n",
    "        return random.choices(range(num_of_actions), weights=self._get_strategy())[0]\n",
    "    def process_training_iteration(self, attacker_action, defender_action):\n",
    "        strategy = self._get_strategy()\n",
    "        for i in range(num_of_actions):\n",
    "            self.strategy_profile_sum[i] += strategy[i]\n",
    "        actual_payoff = self.get_my_payoff(attacker_action, defender_action)\n",
    "        regrets = []\n",
    "        for i in range(num_of_actions):\n",
    "            if self.i_am_attacker:\n",
    "                regret = self.get_my_payoff(i, defender_action)-actual_payoff\n",
    "            else:\n",
    "                regret = self.get_my_payoff(attacker_action, i)-actual_payoff\n",
    "            self.cumulative_regrets[i] += regret\n",
    "    def get_my_payoff(self, attacker_action, defender_action):\n",
    "        multiplier = 1 if self.i_am_attacker else -1\n",
    "        if attacker_action == defender_action:\n",
    "            return 0\n",
    "        # can either sample from score_probability here, or just use score_probability as the value.\n",
    "        # I'm 96% sure we are allowed to do the latter and it should be better, so let's do it\n",
    "        return multiplier * score_probability[attacker_action]\n",
    "    def _get_strategy(self):\n",
    "        nonzero_regrets = [max(0,x) for x in self.cumulative_regrets]\n",
    "        if sum(nonzero_regrets) == 0:\n",
    "            return [1/len(nonzero_regrets) for x in nonzero_regrets]\n",
    "        return [x/sum(nonzero_regrets) for x in nonzero_regrets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using regret minimization:\n",
    "\n",
    "training_iterations = 100000\n",
    "\n",
    "attacker = Learner(i_am_attacker=True)\n",
    "defender = Learner(i_am_attacker=False)\n",
    "for i in range(training_iterations):\n",
    "    a_action = attacker.sample_action()\n",
    "    d_action = defender.sample_action()\n",
    "    \n",
    "    attacker.process_training_iteration(a_action, d_action)\n",
    "    defender.process_training_iteration(a_action, d_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atk probs: ['0.186', '0.159', '0.176', '0.177', '0.142', '0.16']\n",
      "def probs: ['1.67e-06', '0.224', '0.126', '0.125', '0.303', '0.222']\n",
      "expected pts scored per kick: 0.6992083854389095\n"
     ]
    }
   ],
   "source": [
    "# print results:\n",
    "atk_strat = [x/sum(attacker.strategy_profile_sum) for x in attacker.strategy_profile_sum]\n",
    "def_strat = [x/sum(defender.strategy_profile_sum) for x in defender.strategy_profile_sum]\n",
    "print('atk probs:', ['{:.3}'.format(x) for x in atk_strat])\n",
    "print('def probs:', ['{:.3}'.format(x) for x in def_strat])\n",
    "total_payoff = 0\n",
    "for i in range(num_of_actions):\n",
    "    for j in range(num_of_actions):\n",
    "        if i == j:\n",
    "            continue\n",
    "        total_payoff += atk_strat[i] * def_strat[j] * score_probability[i]\n",
    "print('expected pts scored per kick:', total_payoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's compare these values to the ones we get from a browser-based matrix-form nash solver: https://www.math.ucla.edu/~tom/gamesolve.html\n",
    "\n",
    "browser solver:\n",
    "```\n",
    "The matrix is\n",
    " 0 0.7 0.7 0.7 0.7 0.7\n",
    " 0.9 0 0.9 0.9 0.9 0.9\n",
    " 0.8 0.8 0 0.8 0.8 0.8\n",
    " 0.8 0.8 0.8 0 0.8 0.8\n",
    " 1 1 1 1 0 1\n",
    " 0.9 0.9 0.9 0.9 0.9 0\n",
    "The value is 0.69922.\n",
    "An optimal strategy for Player I is:\n",
    " (0.19978,0.15538,0.17481,0.17481,0.13984,0.15538)\n",
    "An optimal strategy for Player II is:\n",
    " (0.00111,0.22309,0.12597,0.12597,0.30078,0.22309)\n",
    "```\n",
    "\n",
    "regret minimization:\n",
    "```\n",
    "atk probs: ['0.182', '0.16', '0.176', '0.176', '0.145', '0.16']\n",
    "def probs: ['2.62e-05', '0.222', '0.125', '0.123', '0.302', '0.227']\n",
    "expected pts scored per kick: 0.6991933761145416\n",
    "```\n",
    "\n",
    "They all seem to be about the same down to 2 decimal points, so I think we're good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so if I used my aforementioned strategy, the most likely scoring probability would be on the 0.8 or 0.7 so I would defend a 0.8 or 0.7.\n",
    "The most likely defending option would be the 1.0, so I would pick a 0.9 to score.\n",
    "\n",
    "[how would I have done?](https://fivethirtyeight.com/features/can-you-construct-the-optimal-tournament/): The scoring is correct (the winners ended up aiming at the top 90%er), but the defending would be off: the best defenders defended the 1.0, not the 0.8 or the 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
