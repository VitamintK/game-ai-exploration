{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1303.4441.pdf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from open_spiel.python.policy import TabularPolicy\n",
    "from open_spiel.python.algorithms import cfr\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "import random\n",
    "from open_spiel.python.algorithms import best_response\n",
    "import copy\n",
    "from open_spiel.python.algorithms import expected_game_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game(\"kuhn_poker\")\n",
    "TRUNK_DEPTH = 3 # the policy for any states with depth < TRUNK_DEPTH are in the trunk.\n",
    "# Anything with depth (move_number) >= TRUNK_DEPTH is considered in subgames\n",
    "# for kuhn poker, trunk_depth = 3 means the first betting action is in the trunk, and the response is a subgame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Solving Strategies in Subgames\n",
    "solve the game, discard the \"subgame strategies\" (i.e. discard the strategies that aren't in the trunk), and then re-solve the subgames.  Compare with unsafe resolving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 500 iterations, exploitability: 0.001168582440478766\n"
     ]
    }
   ],
   "source": [
    "# generate an approximate strategy by running cfr\n",
    "cfr_solver = cfr.CFRSolver(game)\n",
    "ITS = 500\n",
    "for i in range(ITS):\n",
    "    cfr_solver.evaluate_and_update_policy()\n",
    "\n",
    "conv = exploitability.exploitability(game, cfr_solver.average_policy())\n",
    "print(\"After {} iterations, exploitability: {}\".format(i+1, conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedSubgame():\n",
    "    def __init__(self, actual_game, root, chance_outcomes):\n",
    "        self.game = actual_game\n",
    "        self.root = root\n",
    "    def __getattr__(self, attr):\n",
    "        # hacky hacky hacky hacky\n",
    "        assert attr != 'new_initial_state'\n",
    "        return self.game.__getattribute__(attr)\n",
    "    def get_type(self):\n",
    "        overrides = {\n",
    "            'provides_information_state_tensor': False,\n",
    "            'provides_observation_tensor': False,\n",
    "        }\n",
    "        all_attrs = [\"short_name\",\"long_name\",\"dynamics\",\"chance_mode\",\"information\",\"utility\",\"reward_model\",\"max_num_players\",\"min_num_players\",\"provides_information_state_string\",\"provides_information_state_tensor\",\"provides_observation_string\",\"provides_observation_tensor\",\"parameter_specification\"]\n",
    "        kwargs = {k: (self.game.get_type().__getattribute__(k) if k not in overrides else overrides[k]) for k in all_attrs}\n",
    "        return pyspiel.GameType(**kwargs)\n",
    "    def new_initial_state(self):\n",
    "        return self.root\n",
    "    def get(self):\n",
    "        return False\n",
    "\n",
    "glob_id = 0\n",
    "class DummyState:\n",
    "    def __init__(self, game, children, current_player, payoff=None, probabilities=None):\n",
    "        self.game = game\n",
    "        self.cur_player = current_player\n",
    "        self.children = children\n",
    "        self.payoff = payoff\n",
    "        self.probabilities = probabilities\n",
    "        global glob_id\n",
    "        self._id = str(glob_id)\n",
    "        glob_id += 1\n",
    "    def current_player(self):\n",
    "        return self.cur_player\n",
    "    def legal_actions(self, player=None):\n",
    "        if player is not None and player != self.current_player():\n",
    "            return []\n",
    "        return list(range(len(self.children)))\n",
    "    def legal_actions_mask(self, player):\n",
    "        if player is not None and player != self.current_player():\n",
    "            return []\n",
    "        elif self.is_terminal():\n",
    "            return []\n",
    "        else:\n",
    "            length = self.game.max_chance_outcomes() if self.is_chance_node() else self.game.num_distinct_actions()\n",
    "            action_mask = [0] * length\n",
    "            for action in self.legal_actions():\n",
    "                action_mask[action] = 1\n",
    "            return action_mask\n",
    "    def move_number(self):\n",
    "        # HACK: only using this for my own debug_game() function\n",
    "        return 0\n",
    "    def chance_outcomes(self):\n",
    "#         print(list(zip(self.children, self.probabilities)))\n",
    "        assert len(self.probabilities) == len(self.children)\n",
    "        return list(zip(range(len(self.probabilities)), self.probabilities))\n",
    "    def apply_action(self, action):\n",
    "        \"\"\"Applies the specified action to the state.\"\"\"\n",
    "        next_state = self.children[action]\n",
    "        # terrible hack\n",
    "        self.__class__ = next_state.__class__\n",
    "        self.__dict__ = next_state.__dict__\n",
    "    def child(self, action):\n",
    "        return self.children[action]\n",
    "    def is_chance_node(self):\n",
    "        return self.probabilities is not None\n",
    "    def action_to_string(self, arg0, arg1=None):\n",
    "        \"\"\"Action -> string. Args either (player, action) or (action).\"\"\"\n",
    "        player = self.current_player() if arg1 is None else arg0\n",
    "        action = arg0 if arg1 is None else arg1\n",
    "        row, col = self._coord(action)\n",
    "        return \"{}({},{})\".format(\"x\" if player == 0 else \"o\", row, col)\n",
    "    def is_terminal(self):\n",
    "        return len(self.children) == 0\n",
    "    def returns(self):\n",
    "        if self.is_terminal():\n",
    "            return [self.payoff, -self.payoff]\n",
    "        return [0.0, 0.0]\n",
    "    def player_return(self, player):\n",
    "        return self.returns()[player]\n",
    "    def information_state_string(self, pl=None):\n",
    "        if pl is None:\n",
    "            pl = self.current_player()\n",
    "        return 'dummy' + ','.join(x.information_state_string(pl) for x in self.children)\n",
    "    def history_str(self):\n",
    "        hs = 'dummy' + ','.join(x.history_str() for x in self.children) + self._id\n",
    "        return hs\n",
    "    def clone(self):\n",
    "        return copy.deepcopy(self)\n",
    "    def is_simultaneous_node(self):\n",
    "        return False\n",
    "        \n",
    "def get_CBV(br, infostate, player):\n",
    "    # for a state which is not the solving player's decision node\n",
    "    # TODO: make this a method on BestResponsePolicy and submit pull request?\n",
    "    value = 0\n",
    "    print('getting CBV for {} (player {})'.format(infostate, player))\n",
    "    reach_prob_sum = 0\n",
    "    for state in infostate_to_states[(player, infostate)]:\n",
    "        # weight values for how likely it is to reach the state if player plays to get there\n",
    "        print('{}, CF_P: {}, BRV: {}'.format(state.history_str(), history_str_to_reach_probabilities[state.history_str()][1-player], br.value(state)))\n",
    "        value += history_str_to_reach_probabilities[state.history_str()][1-player] * br.value(state)\n",
    "        reach_prob_sum += history_str_to_reach_probabilities[state.history_str()][1-player]\n",
    "    return value/reach_prob_sum\n",
    "def make_augmented_subgame_root(solving_player, roots: [(float,pyspiel.State)]):\n",
    "    \"\"\"\n",
    "    roots is a list of tuples: [(probability, root),] where root is a state at the root of the subgame,\n",
    "    and probability is the probability of getting there.\n",
    "    \"\"\"\n",
    "    other_player = 1-solving_player\n",
    "    root_parents = []\n",
    "    for probability, root in roots:\n",
    "        # todo: can move the BestResponsePolicy construction out of this loop and even out of the function\n",
    "        br = best_response.BestResponsePolicy(game,\n",
    "                                              player_id=other_player,\n",
    "                                              policy=cfr_solver.average_policy(),\n",
    "                                              )\n",
    "        br_value = get_CBV(br, root.information_state_string(other_player), other_player)\n",
    "        print('best response value at {} ({}): {}'.format(root.information_state_string(other_player), root.history_str(), br_value))\n",
    "        alternative_payoff = DummyState(game, children=[], current_player=pyspiel.PlayerId.TERMINAL, payoff=br_value)\n",
    "        root_parent = DummyState(game, children=[alternative_payoff, root], current_player=other_player)\n",
    "        root_parents.append(root_parent)\n",
    "    augmented_subgame_root = DummyState(game, children=root_parents, current_player=-1, probabilities=[x[0] for x in roots])\n",
    "    return augmented_subgame_root\n",
    "    \n",
    "def crawl_game(state, policy):\n",
    "    global infostate_to_states\n",
    "    global history_str_to_reach_probabilities\n",
    "    infostate_to_states = defaultdict(list)\n",
    "    history_str_to_reach_probabilities = dict()\n",
    "    crawl_game_dfs(state, np.array([1, 1]), policy)\n",
    "    \n",
    "def crawl_game_dfs(state, reach_probabilities, policy):\n",
    "    \"\"\"reach_probabilities = [x,y] means a prob of x of getting here if player TWO plays to get here and y prob if p ONE tries to get here\"\"\"\n",
    "    history_str_to_reach_probabilities[state.history_str()] = reach_probabilities\n",
    "    if state.is_terminal():\n",
    "        return\n",
    "    if state.current_player() >= 0:\n",
    "        infostate_to_states[(0,state.information_state_string(0))].append(state)\n",
    "        infostate_to_states[(1,state.information_state_string(1))].append(state)\n",
    "    legal_actions = state.legal_actions()\n",
    "    for action in legal_actions:\n",
    "        new_reach_probabilities = np.array(reach_probabilities)\n",
    "        if state.is_player_node():\n",
    "            new_reach_probabilities[state.current_player()] *= policy.action_probabilities(state)[action]\n",
    "        elif state.is_chance_node():\n",
    "            chance_outcomes = {a:p for a,p in state.chance_outcomes()}\n",
    "            new_reach_probabilities = new_reach_probabilities * chance_outcomes[action]\n",
    "        crawl_game_dfs(state.child(action), new_reach_probabilities, policy)\n",
    "        \n",
    "def get_all_equivalent_states(state):\n",
    "    states = set()\n",
    "    histories = set()\n",
    "    to_explore = [state]\n",
    "    while len(to_explore) > 0:\n",
    "        ex = to_explore.pop()\n",
    "        reach_probabilities = history_str_to_reach_probabilities[ex.history_str()]\n",
    "        states.add((reach_probabilities[state.current_player()], ex))\n",
    "        histories.add(ex.history_str())\n",
    "        # union-find preprocess instead of doing this uberslow search here if you want to do this on bigger games\n",
    "        for infostate in [(player, ex.information_state_string(player)) for player in (0,1)]:\n",
    "            for s in infostate_to_states[infostate]:\n",
    "                if s.history_str() not in histories:\n",
    "                    to_explore.append(s)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history_str_to_reach_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase_subgame_policy_recursive(state, policy):\n",
    "    if state.move_number() >= TRUNK_DEPTH and state.is_player_node():\n",
    "        x = 1/len(s.legal_actions())\n",
    "        for action in s.legal_actions():\n",
    "            policy.policy_for_key(state.information_state_string())[action] = x\n",
    "    for action in state.legal_actions():\n",
    "        erase_subgame_policy_recursive(state.child(action), policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug crawl_game, which should fill up infostate_to_states\n",
    "r = game.new_initial_state()\n",
    "crawl_game(r, cfr_solver.average_policy())\n",
    "#print(infostate_to_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug erase_subgame_policy_recursive, which should erase non-trunk policies from a policy\n",
    "# np.set_printoptions(suppress=True)\n",
    "# trunk_policy = copy.copy(cfr_solver.average_policy())\n",
    "# print(trunk_policy.action_probability_array)\n",
    "# erase_subgame_policy_recursive(game.new_initial_state(), trunk_policy)\n",
    "# print(trunk_policy.action_probability_array)\n",
    "# print('after erasing, exploitable for:', exploitability.nash_conv(game, trunk_policy)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgame_root = game.new_initial_state()\n",
    "subgame_root.apply_action(0)\n",
    "subgame_root.apply_action(1)\n",
    "subgame_root.apply_action(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----the subgame rooted at: 0 1 0 and equivalent states\n",
      "getting CBV for 2p (player 0)\n",
      "2 0 0, CF_P: 0.16666666666666666, BRV: 1.332527195276242\n",
      "2 1 0, CF_P: 0.16666666666666666, BRV: 1.007\n",
      "best response value at 2p (2 0 0): 1.1697635976381209\n",
      "getting CBV for 0p (player 0)\n",
      "0 1 0, CF_P: 0.16666666666666666, BRV: -1.0\n",
      "0 2 0, CF_P: 0.16666666666666666, BRV: -1.0\n",
      "best response value at 0p (0 2 0): -1.0\n",
      "getting CBV for 1p (player 0)\n",
      "1 0 0, CF_P: 0.16666666666666666, BRV: 0.33494560944751584\n",
      "1 2 0, CF_P: 0.16666666666666666, BRV: -1.0\n",
      "best response value at 1p (1 0 0): -0.3325271952762421\n",
      "getting CBV for 1p (player 0)\n",
      "1 0 0, CF_P: 0.16666666666666666, BRV: 0.33494560944751584\n",
      "1 2 0, CF_P: 0.16666666666666666, BRV: -1.0\n",
      "best response value at 1p (1 2 0): -0.3325271952762421\n",
      "getting CBV for 0p (player 0)\n",
      "0 1 0, CF_P: 0.16666666666666666, BRV: -1.0\n",
      "0 2 0, CF_P: 0.16666666666666666, BRV: -1.0\n",
      "best response value at 0p (0 1 0): -1.0\n",
      "getting CBV for 2p (player 0)\n",
      "2 0 0, CF_P: 0.16666666666666666, BRV: 1.332527195276242\n",
      "2 1 0, CF_P: 0.16666666666666666, BRV: 1.007\n",
      "best response value at 2p (2 1 0): 1.1697635976381209\n",
      " his: dummydummydummy0,2 0 01,dummydummy2,0 2 03,dummydummy4,1 0 05,dummydummy6,1 2 07,dummydummy8,0 1 09,dummydummy10,2 1 01112  \n",
      " dummydummydummy0,2 0 01,dummydummy2,0 2 03,dummydummy4,1 0 05,dummydummy6,1 2 07,dummydummy8,0 1 09,dummydummy10,2 1 01112 -> 0 : 0.16666666666666666\n",
      " his: dummydummy0,2 0 01 ,info: dummydummy,2p \n",
      " dummydummy0,2 0 01 -> 0 : 0.8318991790587129\n",
      " his: dummy0  pays out 1.1697635976381209\n",
      " dummydummy0,2 0 01 -> 1 : 0.1681008209412872\n",
      "    his: 2 0 0 ,info: 0p \n",
      "    2 0 0 -> 0 : 0.6693294468561625\n",
      "     his: 2 0 0 0  pays out 1.0\n",
      "    2 0 0 -> 1 : 0.3306705531438376\n",
      "     his: 2 0 0 1 ,info: 2pb \n",
      "     2 0 0 1 -> 0 : 0.0074360136553799985\n",
      "      his: 2 0 0 1 0  pays out -1.0\n",
      "     2 0 0 1 -> 1 : 0.99256398634462\n",
      "      his: 2 0 0 1 1  pays out 2.0\n",
      " dummydummydummy0,2 0 01,dummydummy2,0 2 03,dummydummy4,1 0 05,dummydummy6,1 2 07,dummydummy8,0 1 09,dummydummy10,2 1 01112 -> 1 : 0.16666666666666666\n",
      " his: dummydummy2,0 2 03 ,info: dummydummy,0p \n",
      " dummydummy2,0 2 03 -> 0 : 0.9975\n",
      " his: dummy2  pays out -1.0\n",
      " dummydummy2,0 2 03 -> 1 : 0.0025\n",
      "    his: 0 2 0 ,info: 2p \n",
      "    0 2 0 -> 0 : 0.0025\n",
      "     his: 0 2 0 0  pays out -1.0\n",
      "    0 2 0 -> 1 : 0.9975\n",
      "     his: 0 2 0 1 ,info: 0pb \n",
      "     0 2 0 1 -> 0 : 0.5\n",
      "      his: 0 2 0 1 0  pays out -1.0\n",
      "     0 2 0 1 -> 1 : 0.5\n",
      "      his: 0 2 0 1 1  pays out -2.0\n",
      " dummydummydummy0,2 0 01,dummydummy2,0 2 03,dummydummy4,1 0 05,dummydummy6,1 2 07,dummydummy8,0 1 09,dummydummy10,2 1 01112 -> 2 : 0.16666666666666666\n",
      " his: dummydummy4,1 0 05 ,info: dummydummy,1p \n",
      " dummydummy4,1 0 05 -> 0 : 0.8957313499675021\n",
      " his: dummy4  pays out -0.3325271952762421\n",
      " dummydummy4,1 0 05 -> 1 : 0.10426865003249788\n",
      "    his: 1 0 0 ,info: 0p \n",
      "    1 0 0 -> 0 : 0.6693294468561625\n",
      "     his: 1 0 0 0  pays out 1.0\n",
      "    1 0 0 -> 1 : 0.3306705531438376\n",
      "     his: 1 0 0 1 ,info: 1pb \n",
      "     1 0 0 1 -> 0 : 0.8536800999698702\n",
      "      his: 1 0 0 1 0  pays out -1.0\n",
      "     1 0 0 1 -> 1 : 0.14631990003012993\n",
      "      his: 1 0 0 1 1  pays out 2.0\n",
      " dummydummydummy0,2 0 01,dummydummy2,0 2 03,dummydummy4,1 0 05,dummydummy6,1 2 07,dummydummy8,0 1 09,dummydummy10,2 1 01112 -> 3 : 0.16666666666666666\n",
      " his: dummydummy6,1 2 07 ,info: dummydummy,1p \n",
      " dummydummy6,1 2 07 -> 0 : 0.8957313499675021\n",
      " his: dummy6  pays out -0.3325271952762421\n",
      " dummydummy6,1 2 07 -> 1 : 0.10426865003249788\n",
      "    his: 1 2 0 ,info: 2p \n",
      "    1 2 0 -> 0 : 0.0025\n",
      "     his: 1 2 0 0  pays out -1.0\n",
      "    1 2 0 -> 1 : 0.9975\n",
      "     his: 1 2 0 1 ,info: 1pb \n",
      "     1 2 0 1 -> 0 : 0.8536800999698702\n",
      "      his: 1 2 0 1 0  pays out -1.0\n",
      "     1 2 0 1 -> 1 : 0.14631990003012993\n",
      "      his: 1 2 0 1 1  pays out -2.0\n",
      " dummydummydummy0,2 0 01,dummydummy2,0 2 03,dummydummy4,1 0 05,dummydummy6,1 2 07,dummydummy8,0 1 09,dummydummy10,2 1 01112 -> 4 : 0.16666666666666666\n",
      " his: dummydummy8,0 1 09 ,info: dummydummy,0p \n",
      " dummydummy8,0 1 09 -> 0 : 0.9975\n",
      " his: dummy8  pays out -1.0\n",
      " dummydummy8,0 1 09 -> 1 : 0.0025\n",
      "    his: 0 1 0 ,info: 1p \n",
      "    0 1 0 -> 0 : 0.99\n",
      "     his: 0 1 0 0  pays out -1.0\n",
      "    0 1 0 -> 1 : 0.01\n",
      "     his: 0 1 0 1 ,info: 0pb \n",
      "     0 1 0 1 -> 0 : 0.5\n",
      "      his: 0 1 0 1 0  pays out -1.0\n",
      "     0 1 0 1 -> 1 : 0.5\n",
      "      his: 0 1 0 1 1  pays out -2.0\n",
      " dummydummydummy0,2 0 01,dummydummy2,0 2 03,dummydummy4,1 0 05,dummydummy6,1 2 07,dummydummy8,0 1 09,dummydummy10,2 1 01112 -> 5 : 0.16666666666666666\n",
      " his: dummydummy10,2 1 011 ,info: dummydummy,2p \n",
      " dummydummy10,2 1 011 -> 0 : 0.8318991790587129\n",
      " his: dummy10  pays out 1.1697635976381209\n",
      " dummydummy10,2 1 011 -> 1 : 0.1681008209412872\n",
      "    his: 2 1 0 ,info: 1p \n",
      "    2 1 0 -> 0 : 0.99\n",
      "     his: 2 1 0 0  pays out 1.0\n",
      "    2 1 0 -> 1 : 0.01\n",
      "     his: 2 1 0 1 ,info: 2pb \n",
      "     2 1 0 1 -> 0 : 0.0074360136553799985\n",
      "      his: 2 1 0 1 0  pays out -1.0\n",
      "     2 1 0 1 -> 1 : 0.99256398634462\n",
      "      his: 2 1 0 1 1  pays out 2.0\n",
      "After 200 iterations, exploitability on augmented subgame: 0.0007578081115143659\n",
      "value of subgame: -0.0545947399358864\n",
      "writing to combined strategy\n",
      " at state 0pb, action 0 with probability 0.5\n",
      "writing to combined strategy\n",
      " at state 0pb, action 1 with probability 0.5\n",
      "writing to combined strategy\n",
      " at state 1pb, action 0 with probability 0.8536800999698702\n",
      "writing to combined strategy\n",
      " at state 1pb, action 1 with probability 0.14631990003012993\n",
      "writing to combined strategy\n",
      " at state 2pb, action 0 with probability 0.0074360136553799985\n",
      "writing to combined strategy\n",
      " at state 2pb, action 1 with probability 0.99256398634462\n",
      "writing to combined strategy\n",
      " at state 1p, action 0 with probability 0.99\n",
      "writing to combined strategy\n",
      " at state 1p, action 1 with probability 0.01\n",
      "writing to combined strategy\n",
      " at state 2p, action 0 with probability 0.0025\n",
      "writing to combined strategy\n",
      " at state 2p, action 1 with probability 0.9975\n",
      "writing to combined strategy\n",
      " at state 0p, action 0 with probability 0.6693294468561625\n",
      "writing to combined strategy\n",
      " at state 0p, action 1 with probability 0.3306705531438376\n",
      "----the subgame rooted at: 0 1 1 and equivalent states\n",
      "getting CBV for 2b (player 0)\n",
      "2 0 1, CF_P: 0.16666666666666666, BRV: 1.001\n",
      "2 1 1, CF_P: 0.16666666666666666, BRV: 1.3364958432969685\n",
      "best response value at 2b (2 0 1): 1.1687479216484842\n",
      "getting CBV for 1b (player 0)\n",
      "1 0 1, CF_P: 0.16666666666666666, BRV: 1.001\n",
      "1 2 1, CF_P: 0.16666666666666666, BRV: -1.997\n",
      "best response value at 1b (1 0 1): -0.49800000000000005\n",
      "getting CBV for 1b (player 0)\n",
      "1 0 1, CF_P: 0.16666666666666666, BRV: 1.001\n",
      "1 2 1, CF_P: 0.16666666666666666, BRV: -1.997\n",
      "best response value at 1b (1 2 1): -0.49800000000000005\n",
      "getting CBV for 2b (player 0)\n",
      "2 0 1, CF_P: 0.16666666666666666, BRV: 1.001\n",
      "2 1 1, CF_P: 0.16666666666666666, BRV: 1.3364958432969685\n",
      "best response value at 2b (2 1 1): 1.1687479216484842\n",
      "getting CBV for 0b (player 0)\n",
      "0 1 1, CF_P: 0.16666666666666666, BRV: -0.009487529890905178\n",
      "0 2 1, CF_P: 0.16666666666666666, BRV: -1.997\n",
      "best response value at 0b (0 2 1): -1.0032437649454526\n",
      "getting CBV for 0b (player 0)\n",
      "0 1 1, CF_P: 0.16666666666666666, BRV: -0.009487529890905178\n",
      "0 2 1, CF_P: 0.16666666666666666, BRV: -1.997\n",
      "best response value at 0b (0 1 1): -1.0032437649454526\n",
      " his: dummydummydummy13,2 0 114,dummydummy15,1 0 116,dummydummy17,1 2 118,dummydummy19,2 1 120,dummydummy21,0 2 122,dummydummy23,0 1 12425  \n",
      " dummydummydummy13,2 0 114,dummydummy15,1 0 116,dummydummy17,1 2 118,dummydummy19,2 1 120,dummydummy21,0 2 122,dummydummy23,0 1 12425 -> 0 : 0.16666666666666666\n",
      " his: dummydummy13,2 0 114 ,info: dummydummy,2b \n",
      " dummydummy13,2 0 114 -> 0 : 0.10043312976892635\n",
      " his: dummy13  pays out 1.1687479216484842\n",
      " dummydummy13,2 0 114 -> 1 : 0.8995668702310736\n",
      "    his: 2 0 1 ,info: 0b \n",
      "    2 0 1 -> 0 : 0.9975\n",
      "     his: 2 0 1 0  pays out 1.0\n",
      "    2 0 1 -> 1 : 0.0025\n",
      "     his: 2 0 1 1  pays out 2.0\n",
      " dummydummydummy13,2 0 114,dummydummy15,1 0 116,dummydummy17,1 2 118,dummydummy19,2 1 120,dummydummy21,0 2 122,dummydummy23,0 1 12425 -> 1 : 0.16666666666666666\n",
      " his: dummydummy15,1 0 116 ,info: dummydummy,1b \n",
      " dummydummy15,1 0 116 -> 0 : 0.0025\n",
      " his: dummy15  pays out -0.49800000000000005\n",
      " dummydummy15,1 0 116 -> 1 : 0.9975\n",
      "    his: 1 0 1 ,info: 0b \n",
      "    1 0 1 -> 0 : 0.9975\n",
      "     his: 1 0 1 0  pays out 1.0\n",
      "    1 0 1 -> 1 : 0.0025\n",
      "     his: 1 0 1 1  pays out 2.0\n",
      " dummydummydummy13,2 0 114,dummydummy15,1 0 116,dummydummy17,1 2 118,dummydummy19,2 1 120,dummydummy21,0 2 122,dummydummy23,0 1 12425 -> 2 : 0.16666666666666666\n",
      " his: dummydummy17,1 2 118 ,info: dummydummy,1b \n",
      " dummydummy17,1 2 118 -> 0 : 0.0025\n",
      " his: dummy17  pays out -0.49800000000000005\n",
      " dummydummy17,1 2 118 -> 1 : 0.9975\n",
      "    his: 1 2 1 ,info: 2b \n",
      "    1 2 1 -> 0 : 0.0025\n",
      "     his: 1 2 1 0  pays out 1.0\n",
      "    1 2 1 -> 1 : 0.9975\n",
      "     his: 1 2 1 1  pays out -2.0\n",
      " dummydummydummy13,2 0 114,dummydummy15,1 0 116,dummydummy17,1 2 118,dummydummy19,2 1 120,dummydummy21,0 2 122,dummydummy23,0 1 12425 -> 3 : 0.16666666666666666\n",
      " his: dummydummy19,2 1 120 ,info: dummydummy,2b \n",
      " dummydummy19,2 1 120 -> 0 : 0.10043312976892635\n",
      " his: dummy19  pays out 1.1687479216484842\n",
      " dummydummy19,2 1 120 -> 1 : 0.8995668702310736\n",
      "    his: 2 1 1 ,info: 1b \n",
      "    2 1 1 -> 0 : 0.6514097973762919\n",
      "     his: 2 1 1 0  pays out 1.0\n",
      "    2 1 1 -> 1 : 0.3485902026237081\n",
      "     his: 2 1 1 1  pays out 2.0\n",
      " dummydummydummy13,2 0 114,dummydummy15,1 0 116,dummydummy17,1 2 118,dummydummy19,2 1 120,dummydummy21,0 2 122,dummydummy23,0 1 12425 -> 4 : 0.16666666666666666\n",
      " his: dummydummy21,0 2 122 ,info: dummydummy,0b \n",
      " dummydummy21,0 2 122 -> 0 : 0.7134901471757201\n",
      " his: dummy21  pays out -1.0032437649454526\n",
      " dummydummy21,0 2 122 -> 1 : 0.2865098528242799\n",
      "    his: 0 2 1 ,info: 2b \n",
      "    0 2 1 -> 0 : 0.0025\n",
      "     his: 0 2 1 0  pays out 1.0\n",
      "    0 2 1 -> 1 : 0.9975\n",
      "     his: 0 2 1 1  pays out -2.0\n",
      " dummydummydummy13,2 0 114,dummydummy15,1 0 116,dummydummy17,1 2 118,dummydummy19,2 1 120,dummydummy21,0 2 122,dummydummy23,0 1 12425 -> 5 : 0.16666666666666666\n",
      " his: dummydummy23,0 1 124 ,info: dummydummy,0b \n",
      " dummydummy23,0 1 124 -> 0 : 0.7134901471757201\n",
      " his: dummy23  pays out -1.0032437649454526\n",
      " dummydummy23,0 1 124 -> 1 : 0.2865098528242799\n",
      "    his: 0 1 1 ,info: 1b \n",
      "    0 1 1 -> 0 : 0.6514097973762919\n",
      "     his: 0 1 1 0  pays out 1.0\n",
      "    0 1 1 -> 1 : 0.3485902026237081\n",
      "     his: 0 1 1 1  pays out -2.0\n",
      "After 200 iterations, exploitability on augmented subgame: 0.0032346537282395282\n",
      "value of subgame: -0.10931396938600008\n",
      "writing to combined strategy\n",
      " at state 1b, action 0 with probability 0.6514097973762919\n",
      "writing to combined strategy\n",
      " at state 1b, action 1 with probability 0.3485902026237081\n",
      "writing to combined strategy\n",
      " at state 2b, action 0 with probability 0.0025\n",
      "writing to combined strategy\n",
      " at state 2b, action 1 with probability 0.9975\n",
      "writing to combined strategy\n",
      " at state 0b, action 0 with probability 0.9975\n",
      "writing to combined strategy\n",
      " at state 0b, action 1 with probability 0.0025\n"
     ]
    }
   ],
   "source": [
    "def get_state_probability(state):\n",
    "    \"\"\"how likely is it to get to this state given that the other player tries to get here\"\"\"\n",
    "    pass\n",
    "def get_policy_value(g, policy):\n",
    "    return expected_game_score.policy_value(g.new_initial_state(), [policy] * 2)[0]\n",
    "\n",
    "subgame_solvers = []\n",
    "def debug_game(state, policy):\n",
    "    prefix = ' '*state.move_number()\n",
    "    if not state.is_chance_node() and not state.is_terminal():\n",
    "        info = ',info: ' + state.information_state_string()\n",
    "    else:\n",
    "        info = ''\n",
    "    print(prefix,'his:',state.history_str(), info, 'pays out {}'.format(state.player_return(0)) if state.is_terminal() else '')\n",
    "#     print('state is a chance node:', state.is_chance_node())\n",
    "    for a in state.legal_actions():\n",
    "        if state.is_chance_node():\n",
    "            pass\n",
    "            print(prefix, state.history_str(), '->', a, ':', {a:p for a,p in state.chance_outcomes()}[a])\n",
    "        else:\n",
    "            print(prefix, state.history_str(), '->', a, ':', policy.action_probabilities(state)[a])\n",
    "        debug_game(state.child(a), policy)\n",
    "\n",
    "def train_all_subgames_recursive(state, combined_trunk_subgame_policy, seen):\n",
    "    \"\"\"this method solves subgames, and writes the subgame policies to combined_trunk_subgame_policy.\n",
    "    (combined_trunk_subgame_policy starts off as just the trunk policy)\"\"\"\n",
    "    if state.move_number() >= TRUNK_DEPTH and state.is_player_node():\n",
    "        if state.history_str() in seen:\n",
    "            return\n",
    "        print('----the subgame rooted at:', state.history_str(), 'and equivalent states')\n",
    "        roots = get_all_equivalent_states(state)\n",
    "        for r in roots:\n",
    "            seen.add(r[1].history_str())\n",
    "        augmented_subgame_root = make_augmented_subgame_root(state.current_player(), roots)\n",
    "        augmented_subgame = AugmentedSubgame(game, augmented_subgame_root, len(roots))\n",
    "        subgame_solver = cfr.CFRSolver(augmented_subgame)\n",
    "        ITS = 200\n",
    "        for i in range(ITS):\n",
    "            subgame_solver.evaluate_and_update_policy()\n",
    "        conv = exploitability.nash_conv(augmented_subgame, subgame_solver.average_policy())/2\n",
    "        debug_game(augmented_subgame_root, subgame_solver.average_policy())\n",
    "        subgame_solvers.append(subgame_solver)\n",
    "        print(\"After {} iterations, exploitability on augmented subgame: {}\".format(i+1, conv))\n",
    "        print(\"value of subgame: {}\".format(get_policy_value(augmented_subgame, subgame_solver.average_policy())))\n",
    "        for s in subgame_solver.average_policy().states:\n",
    "            if s.information_state_string() in combined_trunk_subgame_policy.state_lookup:\n",
    "                for action, value in enumerate(subgame_solver.average_policy().policy_for_key(s.information_state_string())):\n",
    "                    print('writing to combined strategy')\n",
    "                    print(' at state {}, action {} with probability {}'.format(s.information_state_string(), action, value))\n",
    "                    combined_trunk_subgame_policy.policy_for_key(s.information_state_string())[action] = value\n",
    "    else:\n",
    "        for action in state.legal_actions():\n",
    "            train_all_subgames_recursive(state.child(action), combined_trunk_subgame_policy, seen)\n",
    "trunk_policy = copy.copy(cfr_solver.average_policy())\n",
    "erase_subgame_policy_recursive(game.new_initial_state(), trunk_policy)\n",
    "seen = set()\n",
    "train_all_subgames_recursive(game.new_initial_state(), trunk_policy, seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird = subgame_solvers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummydummy,2p - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=3, cumulative_regret=defaultdict(<class 'float'>, {0: 0.1980797474949333, 1: -0.7833900177494169}), cumulative_policy=defaultdict(<class 'float'>, {0: 1928.4079485933441, 1: 71.59205140665682}))\n",
      "0p - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=8, cumulative_regret=defaultdict(<class 'float'>, {0: 0.4279504013466878, 1: 0.22399101091802534}), cumulative_policy=defaultdict(<class 'float'>, {0: 1333.7232466304233, 1: 666.276753369541}))\n",
      "2pb - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=2, cumulative_regret=defaultdict(<class 'float'>, {0: -167.31918834238706, 1: 0.25}), cumulative_policy=defaultdict(<class 'float'>, {0: 0.5, 1: 71.09205140665682}))\n",
      "dummydummy,0p - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=4, cumulative_regret=defaultdict(<class 'float'>, {0: 0.041666666666666664, 1: -0.041666666666666664}), cumulative_policy=defaultdict(<class 'float'>, {0: 1999.0, 1: 1.0}))\n",
      "1p - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=6, cumulative_regret=defaultdict(<class 'float'>, {0: 0.049425645336149404, 1: -5.833245305218592}), cumulative_policy=defaultdict(<class 'float'>, {0: 1996.0, 1: 4.0}))\n",
      "0pb - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=0, cumulative_regret=defaultdict(<class 'float'>, {0: 0.08333333333333333, 1: -166.83333333333258}), cumulative_policy=defaultdict(<class 'float'>, {0: 0.5, 1: 0.5}))\n",
      "2p - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=7, cumulative_regret=defaultdict(<class 'float'>, {0: -0.3900780992208588, 1: 0.08333333333333333}), cumulative_policy=defaultdict(<class 'float'>, {0: 1.0, 1: 1999.0}))\n",
      "dummydummy,1p - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=5, cumulative_regret=defaultdict(<class 'float'>, {0: 0.24114828172712577, 1: -0.3875900789893475}), cumulative_policy=defaultdict(<class 'float'>, {0: 1956.4063248532684, 1: 43.59367514673199}))\n",
      "1pb - _InfoStateNode(legal_actions=[0, 1], index_in_tabular_policy=1, cumulative_regret=defaultdict(<class 'float'>, {0: 0.42501122453860923, 1: 0.4108662335931219}), cumulative_policy=defaultdict(<class 'float'>, {0: 37.41273795608161, 1: 6.180937190650312}))\n"
     ]
    }
   ],
   "source": [
    "for i, v in weird._info_state_nodes.items():\n",
    "    print(i, '-', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80812725, 0.19187275],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.98501584, 0.01498416],\n",
       "       [0.85821482, 0.14178518],\n",
       "       [0.42407323, 0.57592677],\n",
       "       [0.00698402, 0.99301598],\n",
       "       [0.998     , 0.002     ],\n",
       "       [0.66127318, 0.33872682],\n",
       "       [0.0005    , 0.9995    ],\n",
       "       [0.0005    , 0.9995    ],\n",
       "       [0.66686162, 0.33313838],\n",
       "       [0.9995    , 0.0005    ]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk_policy.action_probability_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.80812725 0.19187275]\n",
      " [0.99938129 0.00061871]\n",
      " [0.98501584 0.01498416]\n",
      " [0.47621272 0.52378728]\n",
      " [0.42407323 0.57592677]\n",
      " [0.00117904 0.99882096]\n",
      " [0.993      0.007     ]\n",
      " [0.66350416 0.33649584]\n",
      " [0.002      0.998     ]\n",
      " [0.001      0.999     ]\n",
      " [0.6674728  0.3325272 ]\n",
      " [0.999      0.001     ]]\n",
      "['0', '0pb', '1', '1pb', '2', '2pb', '1p', '1b', '2p', '2b', '0p', '0b']\n"
     ]
    }
   ],
   "source": [
    "print(cfr_solver.average_policy().action_probability_array)\n",
    "print([s.information_state_string() for s in cfr_solver.average_policy().states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subgame_solver.average_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09867399924458736\n",
      "0.06500203041284203\n",
      "0.0030334725301733867\n"
     ]
    }
   ],
   "source": [
    "print(exploitability.exploitability(game, trunk_policy))\n",
    "trunk_policy.policy_for_key('0pb')[0] = 1\n",
    "trunk_policy.policy_for_key('0pb')[1] = 0\n",
    "print(exploitability.exploitability(game, trunk_policy))\n",
    "trunk_policy.policy_for_key('1pb')[0] = 0.47621272\n",
    "trunk_policy.policy_for_key('1pb')[1] = 0.52378728\n",
    "print(exploitability.exploitability(game, trunk_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09867399924458736"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they want to know my history 1\n",
      "they want to know my history 0\n"
     ]
    }
   ],
   "source": [
    "tp = p.to_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5 , 0.5 ],\n",
       "       [0.99, 0.01],\n",
       "       [0.5 , 0.5 ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.action_probability_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0542545325460404"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from open_spiel.python import policy\n",
    "\n",
    "game = pyspiel.load_game(\"kuhn_poker\")\n",
    "test_policy = policy.UniformRandomPolicy(game)\n",
    "br = best_response.BestResponsePolicy(game, policy=cfr_solver.average_policy(), player_id=0)\n",
    "br.value(game.new_initial_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspiel.State at 0x7f8b854906f0>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = AugmentedSubgame(game, game.new_initial_state().child(0))\n",
    "test.new_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = game.new_initial_state()\n",
    "s.apply_action(0)\n",
    "s.apply_action(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', '1')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.information_state_string(0), s.information_state_string(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.apply_action(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0p', '1p')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.information_state_string(0), s.information_state_string(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = get_all_equivalent_states(subgame_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<pyspiel.State at 0x7fd82fd50370>,\n",
       " <pyspiel.State at 0x7fd82fd7f230>,\n",
       " <pyspiel.State at 0x7fd8300e5bf0>,\n",
       " <pyspiel.State at 0x7fd82dba1ab0>,\n",
       " <pyspiel.State at 0x7fd8300755f0>,\n",
       " <pyspiel.State at 0x7fd830075130>}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspiel.PlayerAction at 0x7fd82ff57ef0>,\n",
       " <pyspiel.PlayerAction at 0x7fd82fcc3df0>,\n",
       " <pyspiel.PlayerAction at 0x7fd82ff52570>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgame_root.full_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "action_probabilities() missing 1 required positional argument: 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-52aea5192b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: action_probabilities() missing 1 required positional argument: 'state'"
     ]
    }
   ],
   "source": [
    "p.action_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.80812725 0.19187275]\n",
      " [0.99938129 0.00061871]\n",
      " [0.98501584 0.01498416]\n",
      " [0.47621272 0.52378728]\n",
      " [0.42407323 0.57592677]\n",
      " [0.00117904 0.99882096]\n",
      " [0.993      0.007     ]\n",
      " [0.66350416 0.33649584]\n",
      " [0.002      0.998     ]\n",
      " [0.001      0.999     ]\n",
      " [0.6674728  0.3325272 ]\n",
      " [0.999      0.001     ]]\n",
      "[[0.80812725 0.19187275]\n",
      " [0.5        0.5       ]\n",
      " [0.98501584 0.01498416]\n",
      " [0.5        0.5       ]\n",
      " [0.42407323 0.57592677]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]]\n",
      "after erasing, exploitable for: 0.4200273108087714\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0pb', '1', '1pb', '2', '2pb', '1p', '1b', '2p', '2b', '0p', '0b']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[state.information_state_string() for state in fullgamepolicy.states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4200273108087714"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "SpielError",
     "evalue": "/private/var/folders/b4/fpq296zs5yvglb1sb9f660t80000gn/T/pip-install-lxcuryp8/open-spiel/open_spiel/games/kuhn_poker.cc:111 player >= 0\nplayer = -4, 0 = 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSpielError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-d71ba706053e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minformation_state_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSpielError\u001b[0m: /private/var/folders/b4/fpq296zs5yvglb1sb9f660t80000gn/T/pip-install-lxcuryp8/open-spiel/open_spiel/games/kuhn_poker.cc:111 player >= 0\nplayer = -4, 0 = 0"
     ]
    }
   ],
   "source": [
    "s.child(0).information_state_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRUNK_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = cfr_solver.average_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = game.new_initial_state()\n",
    "aaa.apply_action(1)\n",
    "aaa.apply_action(0)\n",
    "aaa.apply_action(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb = game.new_initial_state()\n",
    "bbb.apply_action(1)\n",
    "bbb.apply_action(2)\n",
    "bbb.apply_action(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.8081272519618873, 1: 0.19187274803811277}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf.action_probabilities(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = best_response.BestResponsePolicy(game,\n",
    "                                              player_id=0,\n",
    "                                              policy=cfr_solver.average_policy(),\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-350-3c42777f7dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfosets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1p'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '1p'"
     ]
    }
   ],
   "source": [
    "onep = br.infosets['1p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.value(onep[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cf_p * self.q_value(s, a) for s, cf_p in infoset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<pyspiel.State at 0x7fd8305c6a70>, 1.0)]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(br.decision_nodes(onep[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting CBV for 1p (player 0)\n",
      "1 0 0, CF_P: 0.16666666666666666, BRV: 0.33494560944751584\n",
      "1 2 0, CF_P: 0.16666666666666666, BRV: -1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.11084239842541402"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_CBV(br, '1p', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6674728047237579, 1: 0.3325271952762421}"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfr_solver.average_policy().action_probabilities(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.002, 1: 0.998}"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfr_solver.average_policy().action_probabilities(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.value(bbb.child(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.value(aaa.child(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-393-7a9fb8141356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maaa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/open_spiel/python/policy.py\u001b[0m in \u001b[0;36maction_probabilities\u001b[0;34m(self, state, player_id)\u001b[0m\n\u001b[1;32m    230\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0maction_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;34m\"\"\"Returns an {action: probability} dict, covering all legal actions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_for_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     legal_actions = (state.legal_actions() if player_id is None\n\u001b[1;32m    234\u001b[0m                      else state.legal_actions(player_id))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/open_spiel/python/policy.py\u001b[0m in \u001b[0;36mpolicy_for_key\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \"\"\"\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_probability_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__copy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_action_probability_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0p'"
     ]
    }
   ],
   "source": [
    "p.action_probabilities(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
